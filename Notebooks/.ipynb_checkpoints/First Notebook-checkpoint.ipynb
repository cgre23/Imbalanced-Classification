{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Imbalanced Dataset 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn as skl\n",
    "skl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# plot imbalanced classification problem\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "from numpy import where\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99, 0.01])\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "    row_ix = where(y == label)[0]\n",
    "    pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Imbalanced Classification Models\n",
    "\n",
    "Run it several times and see how the results change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# evaluate imbalanced classification model with different metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0)\n",
    "# split into train/test sets with same class ratio\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, stratify=y)\n",
    "# define model\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "# fit model\n",
    "model.fit(trainX, trainy)\n",
    "# predict on test set\n",
    "yhat = model.predict(testX)\n",
    "# evaluate predictions\n",
    "print('Accuracy: %.3f' % accuracy_score(testy, yhat))\n",
    "print('Precision: %.3f' % precision_score(testy, yhat))\n",
    "print('Recall: %.3f' % recall_score(testy, yhat))\n",
    "print('F-measure: %.3f' % f1_score(testy, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Reference: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "# Explicitly stating labels. Positive=1, Negative=0\n",
    "def true_positive(y_true, y_pred): \n",
    "    return confusion_matrix(y_true, y_pred,labels=[1,0])[0, 0]\n",
    "\n",
    "def true_negative(y_true, y_pred): \n",
    "    return confusion_matrix(y_true,y_pred,labels=[1,0])[1, 1]\n",
    "\n",
    "def false_positive(y_true, y_pred): \n",
    "    return confusion_matrix(y_true, y_pred,labels=[1,0])[1, 0]\n",
    "\n",
    "def false_negative(y_true, y_pred): \n",
    "    return confusion_matrix(y_true, y_pred,labels=[1,0])[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Binary Classifier Metrics\n",
    "# Returns a dictionary {\"MetricName\":Value,...}\n",
    "\n",
    "def binary_classifier_metrics(y_true, y_pred):\n",
    "    metrics = {}\n",
    "\n",
    "    # References: \n",
    "    #  https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "    \n",
    "    # Definition:\n",
    "    # true positive = tp = how many samples were correctly classified as positive (count)\n",
    "    # true negative = tn = how many samples were correctly classified as negative (count)\n",
    "    # false positive = fp = how many negative samples were mis-classified as positive (count)\n",
    "    # false_negative = fn = how many positive samples were mis-classified as negative (count)\n",
    "    \n",
    "    # positive = number of positive samples (count)\n",
    "    #          = true positive + false negative\n",
    "    # negative = number of negative samples (count)\n",
    "    #          = true negative + false positive\n",
    "    \n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    tn = true_negative(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    \n",
    "    positive = tp + fn\n",
    "    negative = tn + fp\n",
    "    \n",
    "    metrics['TruePositive'] = tp\n",
    "    metrics['TrueNegative'] = tn\n",
    "    metrics['FalsePositive'] = fp\n",
    "    metrics['FalseNegative'] = fn\n",
    "    \n",
    "    metrics['Positive'] = positive\n",
    "    metrics['Negative'] = negative\n",
    "    \n",
    "    # True Positive Rate (TPR, Recall) = true positive/positive\n",
    "    # How many positives were correctly classified? (fraction)\n",
    "    # Recall value closer to 1 is better. closer to 0 is worse\n",
    "    if tp == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = tp/positive\n",
    "        \n",
    "    metrics['Recall'] = recall\n",
    "    \n",
    "    # True Negative Rate = True Negative/negative\n",
    "    # How many negatives were correctly classified? (fraction)\n",
    "    # True Negative Rate value closer to 1 is better. closer to 0 is worse\n",
    "    if tn == 0:\n",
    "        tnr = 0\n",
    "    else:\n",
    "        tnr = tn/(negative)\n",
    "    metrics['TrueNegativeRate'] = tnr\n",
    "    \n",
    "    # Precision = True Positive/(True Positive + False Positive)\n",
    "    # How many positives classified by the algorithm are really positives? (fraction)\n",
    "    # Precision value closer to 1 is better. closer to 0 is worse\n",
    "    if tp == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = tp/(tp + fp)\n",
    "    metrics['Precision'] = precision\n",
    "    \n",
    "    # Accuracy = (True Positive + True Negative)/(total positive + total negative)\n",
    "    # How many positives and negatives were correctly classified? (fraction)\n",
    "    # Accuracy value closer to 1 is better. closer to 0 is worse\n",
    "    accuracy = (tp + tn)/(positive + negative)\n",
    "    metrics['Accuracy'] = accuracy\n",
    "    \n",
    "    # False Positive Rate (FPR, False Alarm) = False Positive/(total negative)\n",
    "    # How many negatives were mis-classified as positives (fraction)\n",
    "    # False Positive Rate value closer to 0 is better. closer to 1 is worse\n",
    "    if fp == 0:\n",
    "        fpr = 0\n",
    "    else:\n",
    "        fpr = fp/(negative)\n",
    "    metrics['FalsePositiveRate'] = fpr\n",
    "    \n",
    "    # False Negative Rate (FNR, Misses) = False Negative/(total Positive)\n",
    "    # How many positives were mis-classified as negative (fraction)\n",
    "    # False Negative Rate value closer to 0 is better. closer to 1 is worse\n",
    "    fnr = fn/(positive)\n",
    "    metrics['FalseNegativeRate'] = fnr\n",
    "    \n",
    "    # F1 Score = harmonic mean of Precision and Recall\n",
    "    # F1 Score closer to 1 is better. Closer to 0 is worse.\n",
    "    if precision == 0 or recall == 0:\n",
    "        f1 = 0\n",
    "    else:        \n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "    metrics['F1'] = f1\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classifier_metrics(testy, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for evaluating Imbalanced Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a majority class classifier on an 1:100 imbalanced dataset\n",
    "from numpy import mean\n",
    "from numpy import hstack\n",
    "from numpy import vstack\n",
    "from numpy import where\n",
    "from numpy import unique\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "# create a dataset with a given class distribution\n",
    "def get_dataset(proportions):\n",
    "    # determine the number of classes\n",
    "    n_classes = len(proportions)\n",
    "    # determine the number of examples to generate for each class\n",
    "    largest = max([v for k,v in proportions.items()])\n",
    "    n_samples = largest * n_classes\n",
    "    # create dataset\n",
    "    X, y = make_blobs(n_samples=n_samples, centers=n_classes, n_features=2, random_state=1, cluster_std=3)\n",
    "    # collect the examples\n",
    "    X_list, y_list = list(), list()\n",
    "    for k,v in proportions.items():\n",
    "        row_ix = where(y == k)[0]\n",
    "        selected = row_ix[:v]\n",
    "        X_list.append(X[selected, :])\n",
    "        y_list.append(y[selected])\n",
    "    return vstack(X_list), hstack(y_list)\n",
    "\n",
    "# scatter plot of dataset, different color for each class\n",
    "def plot_dataset(X, y):\n",
    "    # create scatter plot for samples from each class\n",
    "    n_classes = len(unique(y))\n",
    "    for class_value in range(n_classes):\n",
    "        # get row indexes for samples with this class\n",
    "        row_ix = where(y == class_value)[0]\n",
    "        # create scatter of these samples\n",
    "        pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(class_value))\n",
    "    # show a legend\n",
    "    pyplot.legend()\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "    \n",
    "# evaluate a model using repeated k-fold cross-validation\n",
    "def evaluate_model(X, y, metric):\n",
    "    # define model\n",
    "    model = DummyClassifier(strategy='most_frequent')\n",
    "    # evaluate a model with repeated stratified k fold cv\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "# define the class distribution 1:100\n",
    "proportions = {0:10000, 1:1000}\n",
    "# generate dataset\n",
    "X, y = get_dataset(proportions)\n",
    "\n",
    "plot_dataset(X, y)\n",
    "\n",
    "# summarize class distribution:\n",
    "major = (len(where(y == 0)[0]) / len(X)) * 100\n",
    "minor = (len(where(y == 1)[0]) / len(X)) * 100\n",
    "print('Class 0: %.3f%%, Class 1: %.3f%%' % (major, minor))\n",
    "# evaluate model\n",
    "scores = evaluate_model(X, y, 'accuracy')\n",
    "# report score\n",
    "print('Accuracy: %.3f%%' % (mean(scores) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves and AUC in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc curve and auc\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(testy))]\n",
    "# fit a model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "lr_probs = model.predict_proba(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(testy, ns_probs)\n",
    "lr_auc = roc_auc_score(testy, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Curves in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision-recall curve and f1\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from matplotlib import pyplot\n",
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# fit a model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "lr_probs = model.predict_proba(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# predict class values\n",
    "yhat = model.predict(testX)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(testy, lr_probs)\n",
    "lr_f1 = f1_score(testy, yhat)\n",
    "lr_auc  = auc(lr_recall, lr_precision)\n",
    "# summarize scores\n",
    "print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(testy[testy==1]) / len(testy)\n",
    "pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "## function to compute AUC for multiclass\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return roc_auc_score(y_test, y_pred, average=average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute AUC from our results\n",
    "mc_roc_auc = multiclass_roc_auc_score(testy, yhat)\n",
    "print('Multiclass ROC AUC = %.3f'% mc_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### multiclass precision\n",
    "mc_precision = precision_score(testy, yhat, average='macro')\n",
    "print('Multiclass Precision = %.3f'% mc_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### multiclass recall\n",
    "mc_recall = recall_score(testy, yhat, average='macro')\n",
    "print('Multiclass Recall = %.3f'% mc_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute F1 score from  precision and recall\n",
    "def compute_f1(precision, recall):\n",
    "    # F1 Score = harmonic mean of Precision and Recall\n",
    "    # F1 Score closer to 1 is better. Closer to 0 is worse.\n",
    "    if precision == 0 or recall == 0:\n",
    "        return 0\n",
    "    else:        \n",
    "        return 2*precision*recall/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_f1 = compute_f1(mc_precision, mc_recall)\n",
    "print('Multiclass f1 = %.3f'% mc_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
